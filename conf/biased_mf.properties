# recommendation algorithm
rec.recommender.class=net.librec.recommender.baseline.MostPopularRecommender


dfs.data.dir=./myData
dfs.result.dir=../result

#the path to the input data: membership and ratings
data.input.path=rating/rating_1kU_20100_binary.csv
#dfs.membership.dir=membership/membership.csv

# appender class
feature.appender.class=net.librec.data.convertor.appender.ItemFeatureAppender

# path to item feature file
data.itemfeature.path=itemfeature/myitemfeatures.csv

# path to user feature file
data.userfeature.path=userfeature/userfeature.csv

# select the protected feature for appender
data.protected.feature=Africa

#output
data.log.out.path=./result/results.txt
dfs.log.dir=./log

data.column.format=UIR

# setting method of split data
# value can be ratio, loocv, given, KCV
data.model.splitter=ratio
# the ratio of trainset
data.splitter.trainset.ratio=0.8

#or
#data.model.splitter=kcv
#data.splitter.cv.number=5

# using rating to split dataset
data.splitter.ratio=rating
#data.model.format=text

rec.random.seed=201905

# evaluation the result or not
rec.eval.enable=true

# specifies evaluators
# if rec.eval.class is blank
# every evaluator will be calculated
#rec.eval.classes=net.librec.eval.ranking.StatisticalParityEvaluator,net.librec.eval.ranking.ItemCoverageEvaluator
rec.eval.classes=net.librec.eval.ranking.StatisticalParityEvaluator
#rec.eval.classes=sp,icov,ils


# if this algorithm is ranking only true or false
rec.recommender.isranking=true

# can use user,item,social similarity, default value is user, maximum values:user,item,social
rec.recommender.similarities=itemfeature,item
rec.similarity.class=pcc,cos
############################### something is wrong in here!!!!!above!!!
rec.similarity.shrinkage=10

# top k result
rec.recommender.ranking.topn=10

rec.neighbors.knn.number=3
#25/15
rec.neighbors.corate.limit=15

# bold driver: Gemulla et al., Large-scale matrix factorization with distributed stochastic gradient descent, KDD 2011
rec.learnrate.bolddriver=false

# constant decay: Niu et al, Hogwild!: A lock-free approach to parallelizing stochastic gradient descent, NIPS 2011.
rec.learnrate.decay=1.0


rec.iterator.learnrate=0.0002
rec.iterator.learnrate.maximum=0.001
rec.iterator.maximum=2
rec.user.regularization=0.00001
rec.item.regularization=0.00001
rec.bias.regularization=0.02
rec.factor.number=3

